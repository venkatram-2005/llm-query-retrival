
# ğŸ” LLM Queryâ€“Retrieval System using Gemini

A powerful LLM-based system that allows users to upload documents (PDF, DOCX, Emails via URL), ask natural language questions, and get semantically accurate answers with source context.

Built with:

- ğŸ“¦ FastAPI backend
- ğŸ§  Gemini 2.0 Flash for reasoning
- ğŸ§¬ Instructor-XL for embeddings
- âš¡ FAISS for vector similarity search
- ğŸ–¥ï¸ Streamlit frontend for simple UI
- ğŸ“„ Supports PDF, DOCX, and email text

---

## ğŸ—ï¸ Project Structure

```

llm-query-retrieval-gemini/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py                # Loads env vars
â”‚   â”œâ”€â”€ models.py                # Pydantic models
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ query.py             # /hackrx/run endpoint
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ parser.py            # PDF, DOCX, email text extractor
â”‚   â”‚   â”œâ”€â”€ chunker.py           # Semantic chunking
â”‚   â”‚   â”œâ”€â”€ embedder.py          # Embeds using Instructor-XL
â”‚   â”‚   â”œâ”€â”€ vector\_store.py      # FAISS-based retrieval
â”‚   â”‚   â”œâ”€â”€ llm\_decider.py       # Gemini answer engine
â”‚   â”‚   â””â”€â”€ json\_formatter.py    # Formats output
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ downloader.py        # Blob/text downloader
â”‚       â””â”€â”€ logger.py            # Logger
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit\_app.py         # Frontend app
â”‚
â”œâ”€â”€ tests/                       # Unit tests
â”œâ”€â”€ sample\_input.json            # Sample request for testing
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # Env vars (API key, etc.)
â””â”€â”€ README.md                    # This file

````

---

## âš™ï¸ Setup Instructions

### âœ… Prerequisites

- Python 3.10+
- `pip` or `venv` installed
- Virtual environment (recommended)

---

### ğŸš€ Step 1: Clone and Setup

```bash
git clone https://github.com/your-username/llm-query-retrieval-gemini.git
cd llm-query-retrieval-gemini
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
````

---

### ğŸ“¦ Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

---

### ğŸ” Step 3: Create `.env` file

Create a `.env` file in the root directory with:

```env
# API Key from Google AI Console
GEMINI_API_KEY=your_gemini_api_key

# Optional: Set endpoint for frontend to call
API_ENDPOINT=http://localhost:8000/hackrx/run
```

---

### ğŸ§  Step 4: Run FastAPI Backend

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8001
```

ngrok http 8001

It will start at: [http://localhost:8000](http://localhost:8000)

---

### ğŸ’» Step 5: Run Streamlit Frontend

```bash
streamlit run ui/streamlit_app.py
```

Go to: [http://localhost:8501](http://localhost:8501)

---

## ğŸ§ª Running Tests

```bash
pytest tests/
```

---

## ğŸ“¤ Sample Input (JSON)

```json
{
  "documents": "https://example.com/document.pdf",
  "questions": [
    "What is the refund policy?",
    "When does the agreement expire?"
  ]
}
```

---

## ğŸ“Œ Notes

* You can plug in **Google Gemini Pro/Flash** via the API key.
* Supports hierarchical outlines and chunked retrieval.
* Extend to file uploads, vector DB persistence, or multi-modal models.

---

## ğŸ“„ License

MIT Â© 2025 
---

## âœ… Summary: How to Run

| Task         | Command                             |
| ------------ | ----------------------------------- |
| ğŸ”§ Setup     | `pip install -r requirements.txt`   |
| ğŸ§  Backend   | `uvicorn app.main:app --reload`     |
| ğŸ–¥ï¸ Frontend | `streamlit run ui/streamlit_app.py` |
| ğŸ§ª Tests     | `pytest tests/`                     |

---
## Sample Images

### UI
<img width="491" height="578" alt="image" src="https://github.com/user-attachments/assets/323a382d-9018-4856-a175-85b792381ff4" />

